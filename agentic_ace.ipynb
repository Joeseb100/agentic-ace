{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joeseb100/agentic-ace/blob/main/agentic_ace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofAoCMxJTObU",
        "outputId": "18890179-0fa0-42d7-ffa5-f1844bdeeb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… All libraries installed.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \"langchain\" \"langgraph\" \"langchain-openai\" \"langchain_core\" \"python-dotenv\" \"gradio\" \"langchain-groq\" \"langchain-google-genai\" \"langchain-community\"\n",
        "\n",
        "print(\"âœ… All libraries installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "95cc816e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586ad0a8-b177-4e31-ac93-b7c28aee6182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… langchain-community installed.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \"langchain-community\"\n",
        "print(\"âœ… langchain-community installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3545b554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1b15a1-15ca-4201-cdea-87836ccb62bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… langchain-google-genai installed.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \"langchain-google-genai\"\n",
        "print(\"âœ… langchain-google-genai installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vPvQhp-TacJ",
        "outputId": "9c651849-c15a-4f11-91c4-3d0fd732ed7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… API Key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Load Secrets\n",
        "import os\n",
        "from google.colab import userdata\n",
        "try:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    # If using Groq, uncomment the line below instead\n",
        "    # os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "    print(\"âœ… API Key loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"ðŸš¨ Could not load API Key. Please make sure you have set it correctly in the 'Secrets' tab.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWv_XoVFTz5b",
        "outputId": "de608683-2afc-4fc1-b289-32f379dcb1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Directory structure created.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p app/tools\n",
        "print(\"âœ… Directory structure created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "BP2Q6oNf8m5L"
      },
      "outputs": [],
      "source": [
        "!mkdir -p app && touch app/agent.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkqdt3uqUynA",
        "outputId": "a621467a-6a2e-40d0-d939-160314a65ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app/tools/hiring_tools.py\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Write the Hiring Tools file\n",
        "%%writefile app/tools/hiring_tools.py\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def get_application_status(candidate_email: str) -> str:\n",
        "    \"\"\"Looks up the application status for a given candidate email.\"\"\"\n",
        "    print(f\"--- TOOL: Checking status for {candidate_email} ---\")\n",
        "    # In a real system, this would query a database. Here, we fake it.\n",
        "    if \"jane.doe@email.com\" in candidate_email.lower():\n",
        "        return \"Your application is currently under review by the hiring manager. You have advanced to the second round.\"\n",
        "    else:\n",
        "        return \"We could not find an application associated with that email address. Please double-check for typos.\"\n",
        "\n",
        "@tool\n",
        "def lookup_faq(query: str) -> str:\n",
        "    \"\"\"Provides answers to frequently asked questions about the hiring process or company.\"\"\"\n",
        "    print(f\"--- TOOL: Looking up FAQ for '{query}' ---\")\n",
        "    query = query.lower()\n",
        "    if \"prepare\" in query or \"assessment\" in query:\n",
        "        return \"The technical assessment is a 90-minute test on the Assessli platform covering Python and system design. We recommend brushing up on data structures and algorithms.\"\n",
        "    else:\n",
        "        return \"I can't find a specific answer for that. I can help with application status or questions about the technical assessment.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "c_63d_qcU5Qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323ad689-7e13-4b6e-ddac-4e6bb136f4ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app/agent.py\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Write the Agent file\n",
        "%%writefile app/agent.py\n",
        "import os\n",
        "from typing import Annotated, TypedDict\n",
        "from langchain_core.messages import BaseMessage, ToolMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "# from langgraph.prebuilt import ToolExecutor # Removed ToolExecutor import\n",
        "\n",
        "\n",
        "# Choose your model\n",
        "# from langchain_openai import ChatOpenAI\n",
        "# from langchain_groq.chat_models import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "# Import your tools from the file we just created\n",
        "from app.tools.hiring_tools import get_application_status, lookup_faq\n",
        "\n",
        "# 1. Define the state\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, lambda x, y: x + y]\n",
        "\n",
        "# 2. Define the tools\n",
        "tools = [get_application_status, lookup_faq]\n",
        "# Removed ToolExecutor instantiation\n",
        "\n",
        "# 3. Define the model\n",
        "# For best results with tool use, use a larger model.\n",
        "# model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "# model = ChatGroq(model_name=\"llama3-70b-8192\", temperature=0)\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
        "model = model.bind_tools(tools)\n",
        "\n",
        "\n",
        "# 4. Define the nodes\n",
        "def should_continue(state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    # If there is no tool call, then we finish\n",
        "    if not last_message.tool_calls:\n",
        "        return END\n",
        "    # Otherwise if there is, we continue to the tool node\n",
        "    return \"continue\"\n",
        "\n",
        "def call_model(state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "    response = model.invoke(messages)\n",
        "    # We return a list of messages because some of the nodes may return more than one message\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def call_tool(state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    tool_calls = last_message.tool_calls\n",
        "\n",
        "    # We will execute all tool calls in the last message\n",
        "    tool_results = []\n",
        "    for tool_call in tool_calls:\n",
        "        # Find the tool to execute\n",
        "        tool_to_call = {t.name: t for t in tools}[tool_call.name]\n",
        "        # Execute the tool\n",
        "        observation = tool_to_call.invoke(tool_call.args)\n",
        "        tool_results.append(ToolMessage(content=observation, tool_call_id=tool_call.id))\n",
        "\n",
        "    # We return the observations in ToolMessages\n",
        "    return {\"messages\": tool_results}\n",
        "\n",
        "\n",
        "# 5. Define the graph\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "# Define the two nodes we will will include\n",
        "graph.add_node(\"agent\", call_model)\n",
        "graph.add_node(\"action\", call_tool)\n",
        "\n",
        "# Set the entrypoint as the agent\n",
        "graph.set_entry_point(\"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "graph.add_conditional_edges(\n",
        "    # From agent to either itself or the tool node\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        # If the tool node is returned, then we go there\n",
        "        \"continue\": \"action\",\n",
        "        # Otherwise (meaning the latest message is not a tool call), we end\n",
        "        END: END,\n",
        "    },\n",
        ")\n",
        "\n",
        "# We now add a normal edge from the tool node to the agent\n",
        "graph.add_edge(\"action\", \"agent\")\n",
        "\n",
        "app_runnable = graph.compile()\n",
        "\n",
        "# This is the main function we'll call from our UI\n",
        "def execute_agent(message: str):\n",
        "    inputs = {\"messages\": [(\"user\", message)]}\n",
        "    final_state = app_runnable.invoke(inputs)\n",
        "    return final_state[\"messages\"][-1].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "OvE8nVChU_4r",
        "outputId": "afb6ab88-1d8f-481b-a613-b75fcbf20317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Launching ACE Chat Assistant...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c1e197d03664d0d66c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c1e197d03664d0d66c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# Cell 6: Launch the UI\n",
        "import gradio as gr\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the current directory to the Python path\n",
        "sys.path.append(os.path.abspath('.'))\n",
        "\n",
        "from app.agent import execute_agent\n",
        "\n",
        "print(\"ðŸš€ Launching ACE Chat Assistant...\")\n",
        "\n",
        "# Create the Gradio Chat UI\n",
        "# The 'execute_agent' function will be called every time the user sends a message.\n",
        "iface = gr.ChatInterface(\n",
        "    fn=execute_agent,\n",
        "    title=\"ACE - Agentic Candidate Experience\",\n",
        "    description=\"Ask me about your application status (use email 'jane.doe@email.com') or how to prepare for the assessment.\",\n",
        "    examples=[\n",
        "        [\"What is the status of my application for jane.doe@email.com?\"],\n",
        "        [\"How should I prepare for the technical assessment?\"]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(share=True) # share=True creates a public link"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ilTT5cHLIt2X"
      },
      "execution_count": 55,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQnDJVDeR2hejQnqA7Pa2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}